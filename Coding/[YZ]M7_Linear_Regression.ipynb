{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14-kZMzHgvxsw1aPGF_dlYYufz1Fl9lgy","timestamp":1665941470055}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"l0phSUcqjMs4"},"source":["A few key concepts that we will revisit in this walk-through are:\n","\n","1. Apply String Indexer method to find the index of the categorical columns\n","\n","2. Apply OneHot encoding for the categorical columns\n","\n","3. Apply String indexer for the output variable “label” column\n","\n","4. VectorAssembler is applied for both categorical columns and numeric columns. VectorAssembler is a transformer that combines a given list of columns into a single vector column."]},{"cell_type":"markdown","metadata":{"id":"vTrb_YsFDVsT"},"source":["**Step 1**: Install Spark"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yTbkAxL5p1g7","executionInfo":{"status":"ok","timestamp":1679059539178,"user_tz":420,"elapsed":16008,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}},"outputId":"d787f36e-47bd-4f3a-ff3d-81703b98cd34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# innstall java\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# install spark (change the version number if needed)\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n","\n","# unzip the spark file to the current folder\n","!tar xf spark-3.0.0-bin-hadoop3.2.tgz"],"metadata":{"id":"2qY9HjthtVDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""],"metadata":{"id":"J0rkjegDtvZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# install findspark using pip\n","!pip install -q findspark"],"metadata":{"id":"Dmv0qW7YuKuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install pyspark==3.0.2"],"metadata":{"id":"OfQbZI1U6gSA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679059568055,"user_tz":420,"elapsed":24837,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}},"outputId":"bb17ceed-d01a-4e75-87d4-e7f1db2d3f39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark==3.0.2\n","  Downloading pyspark-3.0.2.tar.gz (204.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/204.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9\n","  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186690 sha256=7f573739bfb5231dea41e2aceb3ce2062f51d7d4fc80cd62d22f4ca3a5ec54a2\n","  Stored in directory: /root/.cache/pip/wheels/aa/8e/b9/ed8017fb2997a648f5868a4b728881f320e3d1bd2b0274f137\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.2\n"]}]},{"cell_type":"code","metadata":{"id":"75i_k4uceE1b"},"source":["import findspark\n","findspark.init()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import SparkSession\n","from pyspark.sql import SparkSession\n","# Create a Spark Session\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","# Check Spark Session Information\n","spark\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"Ji03JSROuVcB","executionInfo":{"status":"ok","timestamp":1679059576874,"user_tz":420,"elapsed":8840,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}},"outputId":"a095118f-d215-4d73-cdbe-808d6d692e1c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f7bf80409d0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://f6e713d1a05d:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.0.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"dDgFu5_9eWQp"},"source":["# Create spark_session\n","spark_session = SparkSession.builder.getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T2GIH4gDebm-"},"source":["sc = spark_session.sparkContext"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KDRGW2ZBpCoz"},"source":["# The objective of this exercise is to predict that prices of houses in California"]},{"cell_type":"markdown","metadata":{"id":"wS7OXsgXKOOH"},"source":["**Step 6**: Read in the data file"]},{"cell_type":"code","metadata":{"id":"moTHfFQ-Nv8k"},"source":["# Load in the data\n","rdd = sc.textFile('/content/drive/My Drive/Colab Notebooks/datasets/cal_housing.data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"13NrcBl7oZHT"},"source":["**Step 7**: Read in the headers for the data file read in above"]},{"cell_type":"code","metadata":{"id":"smAt6CrLODcq"},"source":["# Load in the header\n","header = sc.textFile('/content/drive/My Drive/Colab Notebooks/datasets/cal_housing.domain')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fw6IevObojN7"},"source":["**Step 8**: Output the first 2 rows of the data set which was read in Step 7"]},{"cell_type":"code","metadata":{"id":"PD7SDWJAORkb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c1de488-4226-4dcc-e4b6-a8db7122056d","executionInfo":{"status":"ok","timestamp":1679060033403,"user_tz":420,"elapsed":387,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["rdd.take(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000',\n"," '-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"LcC-Q8E7o5JF"},"source":["**Step 9**:  Split the 2 lines on comas and examine the first 2 lines"]},{"cell_type":"code","metadata":{"id":"79fPr5BAs9N_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be4ae381-32bf-44b0-d7ed-5a8a27f15b54","executionInfo":{"status":"ok","timestamp":1679060035144,"user_tz":420,"elapsed":149,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["# Split the lines on commas\n","rdd = rdd.map(lambda line: line.split(\",\"))\n","\n","# Examine the first 2 lines \n","rdd.take(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['-122.230000',\n","  '37.880000',\n","  '41.000000',\n","  '880.000000',\n","  '129.000000',\n","  '322.000000',\n","  '126.000000',\n","  '8.325200',\n","  '452600.000000'],\n"," ['-122.220000',\n","  '37.860000',\n","  '21.000000',\n","  '7099.000000',\n","  '1106.000000',\n","  '2401.000000',\n","  '1138.000000',\n","  '8.301400',\n","  '358500.000000']]"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"wKu6btb7qW6z"},"source":["# Import the necessary modules \n","from pyspark.sql import Row"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PKmYk_gTpmYf"},"source":["**Step 10**: Convert the RDD into a DataFrame - it is easier to work with a DataFrame"]},{"cell_type":"code","metadata":{"id":"2MCS9AR2qYi_"},"source":["# Map the RDD to a DF\n","df = rdd.map(lambda line: Row(longitude=line[0], \n","                              latitude=line[1], \n","                              housingMedianAge=line[2],\n","                              totalRooms=line[3],\n","                              totalBedRooms=line[4],\n","                              population=line[5], \n","                              households=line[6],\n","                              medianIncome=line[7],\n","                              medianHouseValue=line[8])).toDF()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ui5lVKwtqAEn"},"source":["**Step 11**: Display the contents of the DataFrame"]},{"cell_type":"code","metadata":{"id":"JC5XUNqSqhi-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"925abab9-e4eb-44b9-b9c3-32ec18dbdbbb","executionInfo":{"status":"ok","timestamp":1679060044948,"user_tz":420,"elapsed":1506,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["df.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------+----------------+-----------+-------------+-----------+-----------+------------+----------------+\n","|  longitude| latitude|housingMedianAge| totalRooms|totalBedRooms| population| households|medianIncome|medianHouseValue|\n","+-----------+---------+----------------+-----------+-------------+-----------+-----------+------------+----------------+\n","|-122.230000|37.880000|       41.000000| 880.000000|   129.000000| 322.000000| 126.000000|    8.325200|   452600.000000|\n","|-122.220000|37.860000|       21.000000|7099.000000|  1106.000000|2401.000000|1138.000000|    8.301400|   358500.000000|\n","|-122.240000|37.850000|       52.000000|1467.000000|   190.000000| 496.000000| 177.000000|    7.257400|   352100.000000|\n","|-122.250000|37.850000|       52.000000|1274.000000|   235.000000| 558.000000| 219.000000|    5.643100|   341300.000000|\n","|-122.250000|37.850000|       52.000000|1627.000000|   280.000000| 565.000000| 259.000000|    3.846200|   342200.000000|\n","|-122.250000|37.850000|       52.000000| 919.000000|   213.000000| 413.000000| 193.000000|    4.036800|   269700.000000|\n","|-122.250000|37.840000|       52.000000|2535.000000|   489.000000|1094.000000| 514.000000|    3.659100|   299200.000000|\n","|-122.250000|37.840000|       52.000000|3104.000000|   687.000000|1157.000000| 647.000000|    3.120000|   241400.000000|\n","|-122.260000|37.840000|       42.000000|2555.000000|   665.000000|1206.000000| 595.000000|    2.080400|   226700.000000|\n","|-122.250000|37.840000|       52.000000|3549.000000|   707.000000|1551.000000| 714.000000|    3.691200|   261100.000000|\n","|-122.260000|37.850000|       52.000000|2202.000000|   434.000000| 910.000000| 402.000000|    3.203100|   281500.000000|\n","|-122.260000|37.850000|       52.000000|3503.000000|   752.000000|1504.000000| 734.000000|    3.270500|   241800.000000|\n","|-122.260000|37.850000|       52.000000|2491.000000|   474.000000|1098.000000| 468.000000|    3.075000|   213500.000000|\n","|-122.260000|37.840000|       52.000000| 696.000000|   191.000000| 345.000000| 174.000000|    2.673600|   191300.000000|\n","|-122.260000|37.850000|       52.000000|2643.000000|   626.000000|1212.000000| 620.000000|    1.916700|   159200.000000|\n","|-122.260000|37.850000|       50.000000|1120.000000|   283.000000| 697.000000| 264.000000|    2.125000|   140000.000000|\n","|-122.270000|37.850000|       52.000000|1966.000000|   347.000000| 793.000000| 331.000000|    2.775000|   152500.000000|\n","|-122.270000|37.850000|       52.000000|1228.000000|   293.000000| 648.000000| 303.000000|    2.120200|   155500.000000|\n","|-122.260000|37.840000|       50.000000|2239.000000|   455.000000| 990.000000| 419.000000|    1.991100|   158700.000000|\n","|-122.270000|37.840000|       52.000000|1503.000000|   298.000000| 690.000000| 275.000000|    2.603300|   162900.000000|\n","+-----------+---------+----------------+-----------+-------------+-----------+-----------+------------+----------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"fJCdLhh-qI58"},"source":["**Step 12**: Display the data types"]},{"cell_type":"code","metadata":{"id":"TVxAFQ5z9_vT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b555f0e-eb0a-4c75-d43d-cb1ed5c74238","executionInfo":{"status":"ok","timestamp":1679060044949,"user_tz":420,"elapsed":6,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["df.dtypes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('longitude', 'string'),\n"," ('latitude', 'string'),\n"," ('housingMedianAge', 'string'),\n"," ('totalRooms', 'string'),\n"," ('totalBedRooms', 'string'),\n"," ('population', 'string'),\n"," ('households', 'string'),\n"," ('medianIncome', 'string'),\n"," ('medianHouseValue', 'string')]"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"LEl8uRgY-gqi"},"source":["# Import all from `sql.types`\n","from pyspark.sql.types import *\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oUx9WTUcqQ7G"},"source":["**Step 13**: Function that converts the data types of the DataFrame columns"]},{"cell_type":"code","metadata":{"id":"Ur9cxXru-h-4"},"source":["# Write a custom function to convert the data type of DataFrame columns\n","def convertColumn(df, names, newType):\n","  for name in names: \n","     df = df.withColumn(name, df[name].cast(newType))\n","  return df "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4lSI3IW-moT"},"source":["# Assign all column names to `columns`\n","columns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EAtBXGLqhuH"},"source":["**Step 14**: Convert the data types of the above mentioned columns into a float type"]},{"cell_type":"code","metadata":{"id":"dYi07uJn-xsQ"},"source":["from pyspark.sql.types import *\n","# Conver the `df` columns to `FloatType()`\n","df = convertColumn(df, columns, FloatType())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YjL4VkoiqzL5"},"source":["**Step 15**: Confirm that the data type has been converted into float"]},{"cell_type":"code","metadata":{"id":"ktkfgVdF-53_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a73ee595-b0df-4ef0-f151-fe688ff5df1c","executionInfo":{"status":"ok","timestamp":1679060051814,"user_tz":420,"elapsed":164,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["df.dtypes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('longitude', 'float'),\n"," ('latitude', 'float'),\n"," ('housingMedianAge', 'float'),\n"," ('totalRooms', 'float'),\n"," ('totalBedRooms', 'float'),\n"," ('population', 'float'),\n"," ('households', 'float'),\n"," ('medianIncome', 'float'),\n"," ('medianHouseValue', 'float')]"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"EF2Ff0P-CWn4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"186e3ab9-07a6-447e-8a3e-c28d20e8b7bf","executionInfo":{"status":"ok","timestamp":1679060053456,"user_tz":420,"elapsed":2,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["# Print the schema of `df`\n","df.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- longitude: float (nullable = true)\n"," |-- latitude: float (nullable = true)\n"," |-- housingMedianAge: float (nullable = true)\n"," |-- totalRooms: float (nullable = true)\n"," |-- totalBedRooms: float (nullable = true)\n"," |-- population: float (nullable = true)\n"," |-- households: float (nullable = true)\n"," |-- medianIncome: float (nullable = true)\n"," |-- medianHouseValue: float (nullable = true)\n","\n"]}]},{"cell_type":"code","metadata":{"id":"PcIqdw1j-7-s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e6b7b4f-56bf-41b9-cb68-473f6fe7e8b8","executionInfo":{"status":"ok","timestamp":1679060058252,"user_tz":420,"elapsed":3100,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["df.describe().show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n","|summary|          longitude|          latitude|  housingMedianAge|        totalRooms|    totalBedRooms|        population|        households|      medianIncome|  medianHouseValue|\n","+-------+-------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n","|  count|              20640|             20640|             20640|             20640|            20640|             20640|             20640|             20640|             20640|\n","|   mean|-119.56970444871473| 35.63186143109965|28.639486434108527|2635.7630813953488|537.8980135658915|1425.4767441860465| 499.5396802325581|3.8706710030346416|206855.81690891474|\n","| stddev| 2.0035317429328914|2.1359523806029554|12.585557612111613|2181.6152515827994|421.2479059431315|1132.4621217653385|382.32975283161136|1.8998217183639672|115395.61587441381|\n","|    min|            -124.35|             32.54|               1.0|               2.0|              1.0|               3.0|               1.0|            0.4999|           14999.0|\n","|    max|            -114.31|             41.95|              52.0|           39320.0|           6445.0|           35682.0|            6082.0|           15.0001|          500001.0|\n","+-------+-------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"pCw-Zj57aKal"},"source":["\n","You should probably standardize your data, as you have seen that the range of minimum and maximum values is quite large.\n","\n","Your dependent variable is also quite large; you should adjust the values slightly."]},{"cell_type":"markdown","metadata":{"id":"TI9sdf3QrX13"},"source":["**Step 16**: Processing the data"]},{"cell_type":"code","metadata":{"id":"7irQ2v75aMDA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f2c6ab1-83b7-4543-b90d-c77516e74cb0","executionInfo":{"status":"ok","timestamp":1679060058764,"user_tz":420,"elapsed":517,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["# Import all from `sql.functions` \n","from pyspark.sql.functions import *\n","\n","# Adjust the values of `medianHouseValue` - surface the house values in units of 100,000\n","df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n","\n","# Show the first 2 lines of `df`\n","df.take(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(longitude=-122.2300033569336, latitude=37.880001068115234, housingMedianAge=41.0, totalRooms=880.0, totalBedRooms=129.0, population=322.0, households=126.0, medianIncome=8.325200080871582, medianHouseValue=4.526),\n"," Row(longitude=-122.22000122070312, latitude=37.86000061035156, housingMedianAge=21.0, totalRooms=7099.0, totalBedRooms=1106.0, population=2401.0, households=1138.0, medianIncome=8.301400184631348, medianHouseValue=3.585)]"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"bprstdLLdvUj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d06cd29-2b5d-4b56-f8af-48ccccbdfa01","executionInfo":{"status":"ok","timestamp":1679060062863,"user_tz":420,"elapsed":697,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["from pyspark.sql.functions import *\n","\n","# Divide `totalRooms` by `households`\n","roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n","\n","# Divide `population` by `households`\n","populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n","\n","# Divide `totalBedRooms` by `totalRooms`\n","bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n","\n","# Add the new columns to `df`\n","df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n","   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n","   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n","   \n","# Inspect the result\n","df.first()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Row(longitude=-122.2300033569336, latitude=37.880001068115234, housingMedianAge=41.0, totalRooms=880.0, totalBedRooms=129.0, population=322.0, households=126.0, medianIncome=8.325200080871582, medianHouseValue=4.526, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"UPu4mhyZd9H7"},"source":["# Re-order and select columns\n","df = df.select(\"medianHouseValue\", \n","              \"totalBedRooms\", \n","              \"population\", \n","              \"households\", \n","              \"medianIncome\", \n","              \"roomsPerHousehold\", \n","              \"populationPerHousehold\", \n","              \"bedroomsPerRoom\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.show(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJ2fxmpQZUks","executionInfo":{"status":"ok","timestamp":1679060066363,"user_tz":420,"elapsed":715,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}},"outputId":"4b0e4233-6ca5-433a-9a01-d88abf5429fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------+-------------+----------+----------+------------+------------------+----------------------+-------------------+\n","|medianHouseValue|totalBedRooms|population|households|medianIncome| roomsPerHousehold|populationPerHousehold|    bedroomsPerRoom|\n","+----------------+-------------+----------+----------+------------+------------------+----------------------+-------------------+\n","|           4.526|        129.0|     322.0|     126.0|      8.3252| 6.984126984126984|    2.5555555555555554|0.14659090909090908|\n","|           3.585|       1106.0|    2401.0|    1138.0|      8.3014| 6.238137082601054|     2.109841827768014|0.15579659106916466|\n","|           3.521|        190.0|     496.0|     177.0|      7.2574| 8.288135593220339|    2.8022598870056497|0.12951601908657123|\n","|           3.413|        235.0|     558.0|     219.0|      5.6431|5.8173515981735155|     2.547945205479452|0.18445839874411302|\n","|           3.422|        280.0|     565.0|     259.0|      3.8462| 6.281853281853282|    2.1814671814671813| 0.1720958819913952|\n","|           2.697|        213.0|     413.0|     193.0|      4.0368| 4.761658031088083|     2.139896373056995|0.23177366702937977|\n","|           2.992|        489.0|    1094.0|     514.0|      3.6591|4.9319066147859925|    2.1284046692607004|0.19289940828402366|\n","|           2.414|        687.0|    1157.0|     647.0|        3.12| 4.797527047913447|    1.7882534775888717|0.22132731958762886|\n","|           2.267|        665.0|    1206.0|     595.0|      2.0804| 4.294117647058823|     2.026890756302521| 0.2602739726027397|\n","|           2.611|        707.0|    1551.0|     714.0|      3.6912| 4.970588235294118|     2.172268907563025| 0.1992110453648915|\n","+----------------+-------------+----------+----------+------------+------------------+----------------------+-------------------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"p5qXurv_rhD8"},"source":["**Step 17**: Specifying the label and the features - the label in this case is the dependent variable i.e. **medianHouseValue**"]},{"cell_type":"code","metadata":{"id":"xzeatbBae8ou"},"source":["# Import `DenseVector`\n","# A Dense Vector is used to store arrays of values for use in PySpark.\n","from pyspark.ml.linalg import DenseVector\n","\n","# Define the `input_data` \n","input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n","\n","# Replace `df` with the new DataFrame\n","df = spark_session.createDataFrame(input_data, [\"label\", \"features\"])\n","\n","label = df.rdd.map(lambda x: x.label)\n","features = df.rdd.map(lambda x: x.features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(input_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MMLBDmveXYWo","executionInfo":{"status":"ok","timestamp":1679060072327,"user_tz":420,"elapsed":144,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}},"outputId":"48d4b9b8-8e57-4ce1-ad6f-71d5dc2232e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PythonRDD[50] at RDD at PythonRDD.scala:53\n"]}]},{"cell_type":"markdown","metadata":{"id":"Kcv_86EQrxY3"},"source":["**Step 18**: Scaling the features using 'StandardScaler' - standardizes a feature of the model by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation.\n","\n","**Additional Information**: https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02"]},{"cell_type":"code","metadata":{"id":"4cLUbFwue-Vh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"62d4df78-41c8-47b7-b5d5-41465495c83a","executionInfo":{"status":"ok","timestamp":1679060077119,"user_tz":420,"elapsed":3457,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["# Import `StandardScaler` \n","from pyspark.ml.feature import StandardScaler\n","\n","# Initialize the `standardScaler`\n","standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n","\n","# Fit the DataFrame to the scaler\n","scaler = standardScaler.fit(df.select('features'))\n","\n","# Transform the data in `df` with the scaler\n","scaled_df = scaler.transform(df)\n","\n","# Inspect the result\n","scaled_df.take(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])),\n"," Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"kCvDd1rYsgCC"},"source":["**Step 19**: Create the \"Train/Test\" split"]},{"cell_type":"code","metadata":{"id":"DoGsoLK_fLpj"},"source":["# Split the data into train and test sets\n","train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JieYBFnefQBa"},"source":["# Import `LinearRegression`\n","from pyspark.ml.regression import LinearRegression\n","\n","# Initialize `lr`\n","lr = LinearRegression(labelCol=\"label\", maxIter=100, regParam=0.3, elasticNetParam=0.8)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0S7R6ew6CqGL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6c3f171-beef-409f-87cd-1f29e6e315fc","executionInfo":{"status":"ok","timestamp":1679060080161,"user_tz":420,"elapsed":1922,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["train_data.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+--------------------+--------------------+\n","|  label|            features|     features_scaled|\n","+-------+--------------------+--------------------+\n","|0.14999|[73.0,85.0,38.0,1...|[0.17329463000311...|\n","|0.14999|[267.0,628.0,225....|[0.63383104398397...|\n","|  0.175|[168.0,259.0,138....|[0.39881503891126...|\n","|   0.25|[33.0,64.0,27.0,0...|[0.07833866835757...|\n","|  0.266|[309.0,808.0,294....|[0.73353480371179...|\n","|  0.269|[543.0,1423.0,482...|[1.28902717933820...|\n","|    0.3|[183.0,500.0,177....|[0.43442352452834...|\n","|    0.3|[448.0,338.0,182....|[1.06350677043004...|\n","|  0.325|[49.0,51.0,20.0,4...|[0.11632105301578...|\n","|  0.329|[386.0,436.0,213....|[0.91632502987946...|\n","|  0.332|[131.0,511.0,124....|[0.31098077438914...|\n","|  0.344|[121.0,530.0,115....|[0.28724178397775...|\n","|  0.346|[208.0,660.0,188....|[0.49377100055680...|\n","|  0.366|[199.0,567.0,204....|[0.47240590918656...|\n","|  0.367|[238.0,425.0,157....|[0.56498797179096...|\n","|  0.375|[20.0,39.0,18.0,3...|[0.04747798082276...|\n","|  0.375|[194.0,573.0,134....|[0.46053641398086...|\n","|  0.379|[756.0,1798.0,749...|[1.79466767510070...|\n","|  0.388|[103.0,470.0,96.0...|[0.24451160123726...|\n","|  0.392|[237.0,772.0,220....|[0.56261407274982...|\n","+-------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"CdcazpdrDRWN"},"source":["# Fit the data to the model\n","linearModel = lr.fit(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jXQe3IMXwP5A"},"source":["**Step 20**: Make the predictions"]},{"cell_type":"code","metadata":{"id":"xVETHzzdc-DX"},"source":["# Make predictions on test data\n","predicted = linearModel.transform(test_data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_at1Sm-fdH8E"},"source":["# Retrieve the predictions and the \"known\" labels\n","predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n","labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6D08VCHdMAm"},"source":["# Combine the predictions and the label\n","predictionAndLabel = predictions.zip(labels).collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iM07ePZ_xzoD"},"source":["**Step 21**: Output the predictions and the associated labels"]},{"cell_type":"code","metadata":{"id":"dZz1ke1wdQ2c","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6af6fb9-e7ad-45e4-ef54-b8409a9499e6","executionInfo":{"status":"ok","timestamp":1679060094955,"user_tz":420,"elapsed":30,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["# Print out first 5 instances of `predictionAndLabel` - Please note that the medianHouseValue was divided by 100000 in Step 17\n","predictionAndLabel[:15]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1.5755376157988699, 0.14999),\n"," (1.7341249044713924, 0.225),\n"," (1.5318778705905935, 0.388),\n"," (1.362865204899986, 0.394),\n"," (1.286302045413713, 0.396),\n"," (1.5233279429128062, 0.398),\n"," (1.4606101380914116, 0.4),\n"," (1.3705877149188916, 0.431),\n"," (1.3286655270672303, 0.44),\n"," (1.35619075944089, 0.441),\n"," (1.2721532889306606, 0.45),\n"," (1.3940310618213674, 0.455),\n"," (1.3423454047760115, 0.455),\n"," (1.4891558500524658, 0.455),\n"," (1.5159364196844893, 0.463)]"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"ZbrbTry1H12m"},"source":["**Step 22**: Evaluating the model"]},{"cell_type":"markdown","metadata":{"id":"HhntEZ4PJxu_"},"source":["**RMSE**: RMSE measures the differences between predicted values by the model and the actual values.The smaller the RMSE value is, the closer predicted and actual values are."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HAY7zu3fJq3W","outputId":"9f758947-a4ce-4b6c-a2c9-ff34a427d73a","executionInfo":{"status":"ok","timestamp":1679060094955,"user_tz":420,"elapsed":26,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["linearModel.summary.rootMeanSquaredError"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8757283763853735"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"uE8R7KvCKGhQ"},"source":["**R-Squared** known as \"Co-efficient of determination\" illustrates the extent of the variability in the \"MedianHouseValue\" that can be explained by the Linear Regression model. The higher the R-squared, the better the model fits the underlying data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKAwTryaLGdc","outputId":"710945f8-d35c-4c3a-d507-702db834e65d","executionInfo":{"status":"ok","timestamp":1679060094956,"user_tz":420,"elapsed":22,"user":{"displayName":"Yu Zhang","userId":"15502925183467796581"}}},"source":["linearModel.summary.r2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.42060409588578673"]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"hNApnBDdLVG8"},"source":["Only 42% of the variability in the \"MedianHouseValues\" is explained by the Linear Regression model.There is definitely room for improvement. You can play around with the parameters that you passed to your model."]}]}